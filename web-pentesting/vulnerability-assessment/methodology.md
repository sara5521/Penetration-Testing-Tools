# üìã Vulnerability Assessment Methodology

Comprehensive methodology for conducting systematic vulnerability assessments in penetration testing engagements.

**Location in Framework:** `web-pentesting/vulnerability-assessment/methodology.md`

---

## üéØ Overview

Vulnerability assessment is a systematic process of identifying, quantifying, and prioritizing security vulnerabilities in systems, applications, and networks. This methodology ensures comprehensive coverage and consistent results.

---

## üîÑ Assessment Lifecycle

### Phase 1: Planning and Scoping
- **Define objectives** and success criteria
- **Identify target systems** and applications
- **Establish testing boundaries** and constraints
- **Document approval** and authorization
- **Select appropriate tools** and techniques

### Phase 2: Information Gathering
- **Passive reconnaissance** - OSINT, DNS enumeration
- **Active reconnaissance** - Port scanning, service detection
- **Application mapping** - Crawling, endpoint discovery
- **Technology fingerprinting** - Server, framework identification

### Phase 3: Vulnerability Identification
- **Automated scanning** - Tool-based detection
- **Manual verification** - Confirm automated findings
- **Configuration review** - Security misconfigurations
- **Code analysis** - Source code vulnerabilities (if applicable)

### Phase 4: Vulnerability Analysis
- **Risk assessment** - Impact and likelihood evaluation
- **False positive elimination** - Verify actual vulnerabilities
- **Exploit development** - Proof of concept creation
- **Impact demonstration** - Show real-world consequences

### Phase 5: Reporting and Remediation
- **Findings documentation** - Detailed vulnerability reports
- **Risk prioritization** - Critical to informational ranking
- **Remediation guidance** - Specific fix recommendations
- **Executive summary** - Business impact communication

---

## üß™ Assessment Categories

### Network Infrastructure Assessment
```
Scope: Network devices, services, protocols
Focus: Configuration, access controls, encryption
Tools: Nmap, Nessus, OpenVAS
Methods: Port scanning, service enumeration, protocol analysis
```

### Web Application Assessment
```
Scope: Web applications, APIs, web services
Focus: Input validation, authentication, session management
Tools: Burp Suite, OWASP ZAP, Nikto, WMAP
Methods: Manual testing, automated scanning, code review
```

### Wireless Network Assessment
```
Scope: WiFi networks, access points, wireless protocols
Focus: Encryption, authentication, access controls
Tools: Aircrack-ng, Kismet, WiFi Pineapple
Methods: Passive monitoring, active attacks, configuration review
```

### Database Assessment
```
Scope: Database servers, configurations, access controls
Focus: Authentication, authorization, data protection
Tools: SQLMap, Nmap NSE scripts, database scanners
Methods: Configuration review, injection testing, privilege analysis
```

---

## üìä Vulnerability Classification

### OWASP Risk Rating Model
```
Risk = Likelihood √ó Impact

Likelihood Factors:
- Threat agent factors (skill level, motive, opportunity)
- Vulnerability factors (ease of discovery, exploitation)

Impact Factors:
- Technical impact (loss of confidentiality, integrity, availability)
- Business impact (financial damage, reputation loss, compliance)
```

### CVSS (Common Vulnerability Scoring System)
```
Base Score Components:
- Attack Vector (Network, Adjacent, Local, Physical)
- Attack Complexity (Low, High)
- Privileges Required (None, Low, High)
- User Interaction (None, Required)
- Confidentiality Impact (None, Low, High)
- Integrity Impact (None, Low, High)
- Availability Impact (None, Low, High)
```

### Risk Levels
| Risk Level | CVSS Score | Description | Response Time |
|------------|------------|-------------|---------------|
| **Critical** | 9.0 - 10.0 | Immediate threat to business | 24-48 hours |
| **High** | 7.0 - 8.9 | Significant security risk | 1-2 weeks |
| **Medium** | 4.0 - 6.9 | Moderate security concern | 1-3 months |
| **Low** | 0.1 - 3.9 | Minor security issue | Next maintenance window |

---

## üõ†Ô∏è Tool Selection Matrix

### Automated Scanners
| Tool | Scope | Strengths | Limitations |
|------|-------|-----------|-------------|
| **Nessus** | Network/Web | Comprehensive, accurate | Commercial, resource intensive |
| **OpenVAS** | Network/Web | Free, extensive plugins | Complex setup, false positives |
| **Burp Suite** | Web applications | Manual verification, extensible | Web-focused only |
| **Nikto** | Web servers | Fast, signature-based | Limited scope, false positives |

### Manual Testing Tools
| Tool | Purpose | Use Case |
|------|---------|----------|
| **Nmap** | Network discovery | Port scanning, service detection |
| **Metasploit** | Exploitation | Proof of concept development |
| **SQLMap** | SQL injection | Database exploitation |
| **Dirb/Gobuster** | Directory enumeration | Hidden content discovery |

---

## üìã Assessment Checklist

### Pre-Assessment
- [ ] Scope definition and approval obtained
- [ ] Testing environment prepared
- [ ] Tools configured and updated
- [ ] Backup and rollback plans created
- [ ] Communication channels established

### During Assessment
- [ ] All target systems identified and cataloged
- [ ] Automated scans completed and results reviewed
- [ ] Manual verification performed for critical findings
- [ ] Proof of concept developed for exploitable vulnerabilities
- [ ] Evidence collected and documented
- [ ] Progress communicated to stakeholders

### Post-Assessment
- [ ] All findings verified and validated
- [ ] Risk ratings assigned based on impact and likelihood
- [ ] Detailed technical report created
- [ ] Executive summary prepared
- [ ] Remediation recommendations provided
- [ ] Follow-up testing scheduled

---

## üéØ eJPT Assessment Focus

### Core Assessment Skills
1. **Network enumeration** using Nmap and NSE scripts
2. **Web application testing** with Burp Suite and automated tools
3. **Vulnerability validation** through manual verification
4. **Risk assessment** using standard frameworks
5. **Documentation** of findings and evidence

### Common eJPT Scenarios
- **Web application vulnerabilities** (SQL injection, XSS, file upload)
- **Network service misconfigurations** (default credentials, weak encryption)
- **Information disclosure** (directory traversal, sensitive files)
- **Authentication bypasses** (weak passwords, session management)

### Key Assessment Areas
```bash
# Network assessment
nmap -sS -sV -sC -p- target.com

# Web application assessment  
nikto -h http://target.com
wmap_run -t (in Metasploit)

# Manual verification
burpsuite # Intercept and test manually
```

---

## üìà Quality Assurance

### Validation Techniques
- **Cross-tool verification** - Confirm findings with multiple tools
- **Manual validation** - Manually verify automated findings
- **Peer review** - Have colleagues review critical findings
- **Client confirmation** - Verify findings in production environment

### False Positive Management
```
Common False Positives:
- SSL/TLS certificate warnings on internal systems
- Default content detection on customized applications
- Version-based vulnerabilities without actual exploitation
- Network service banners without configuration review
```

### Quality Metrics
- **Coverage percentage** - Proportion of attack surface tested
- **Accuracy rate** - Ratio of valid findings to total findings
- **Critical finding verification** - All high/critical findings manually confirmed
- **Time efficiency** - Balance between thoroughness and timeline

---

## üìã Reporting Standards

### Technical Report Structure
1. **Executive Summary**
   - Risk overview and business impact
   - Key findings summary
   - Remediation priority matrix

2. **Methodology**
   - Assessment scope and approach
   - Tools and techniques used
   - Testing timeline and limitations

3. **Detailed Findings**
   - Vulnerability descriptions
   - Technical details and evidence
   - Risk ratings and impact analysis
   - Specific remediation steps

4. **Appendices**
   - Raw scan results
   - Screenshots and proof of concept
   - Reference materials and standards

### Evidence Collection Standards
- **Screenshots** with timestamps and context
- **Command output** showing exploitation steps
- **Network traffic** captures when relevant
- **Log entries** demonstrating impact
- **Before/after** comparisons for verification

---

## üîÑ Continuous Improvement

### Assessment Metrics
- Track finding accuracy over time
- Monitor remediation success rates
- Measure assessment efficiency
- Collect client feedback

### Knowledge Management
- Maintain vulnerability database
- Document new attack techniques
- Update tool configurations
- Share lessons learned across team

### Skill Development
- Regular training on new vulnerabilities
- Tool proficiency development
- Industry certification maintenance
- Participation in security communities
